{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YH-ikFFI6Nr2"
      ],
      "authorship_tag": "ABX9TyMwAES5MsEtxUYOEQX9Z0hN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrin2202/NLP/blob/main/HW_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Работа с объектами-контейнерами и настройками spaCy под свои нужды"
      ],
      "metadata": {
        "id": "YH-ikFFI6Nr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Объекты-контейнеры бибилиотеки spaCy"
      ],
      "metadata": {
        "id": "S7r1JzRZYfVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Получение индекса токена в объекте Doc"
      ],
      "metadata": {
        "id": "nuKfutcD6bKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens.doc import Doc\n",
        "from spacy.vocab import Vocab"
      ],
      "metadata": {
        "id": "KD7DbfB068F7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь вызываем конструктор класса `Doc` и передаем ему два параметра: объект `vocab` — контейнер хранилища со словарными данными и список токенов для добавления в создаваемый объект `Doc`"
      ],
      "metadata": {
        "id": "MR46jSpk7VFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Doc(Vocab(), words=[u'Hi', u'there'])\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udb1rqi-6lmE",
        "outputId": "69c2e29a-4fe1-4ba8-d1ba-7384d2a53853"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hi there "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обход в цикле синтаксических дочерних элементов токена"
      ],
      "metadata": {
        "id": "puW9vLMO7wnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам потребуется установить и загрузать пакет обученного конвейера по умолчанию"
      ],
      "metadata": {
        "id": "0A0j1pFY9D-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-fPPY1Q82aK",
        "outputId": "9164d3c4-1d54-4527-8159-22400b8135da"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "s0uO4J2W8tgS"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для получения программным образом левосторонних дочерних элементов токена *apple* в данном предложении можно воспользоваться следующим кодом:"
      ],
      "metadata": {
        "id": "YfNehPL89LlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I want a green apple.\")\n",
        "[w for w in doc[4].lefts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KdepNOf7SAi",
        "outputId": "720d9c59-6f58-451e-a2b3-16bd0be0150f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a, green]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "У слова *apple* есть только левосторонние синтаксические дочерние элементы. На практике это означает, что можно заменить атрибут `Token.lefts` на `Token.children`, служащий для поиска всех дочерних элементов токена:"
      ],
      "metadata": {
        "id": "JTyIp8fo9mIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in doc[4].children]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAeMb29677IC",
        "outputId": "12ac00f7-e820-4f00-dff6-0ecb13d565ec"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[a, green]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно также использовать атрибут `Token.rights` для получения правосторонних синтаксических дочерних элементов:\n",
        "Но слово *apply* само является правосторонним дочерним элементом слова want"
      ],
      "metadata": {
        "id": "dRl7SR4j90BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in doc[4].rights]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfxHLeqo9sOn",
        "outputId": "f1c9784a-31d6-450b-b49f-a78ab8b33e91"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Контейнер `doc.sents`"
      ],
      "metadata": {
        "id": "Q-9L6S4l-IAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cмысл синтаксических меток, присваиваемых токенам, проявляется лишь в контексте предложения, где встречается данный токен.\n",
        "С помощью свойства `doc.sents` объекта `Doc` текст можно разделить на отдельные предложения, как показано в следующем примере:"
      ],
      "metadata": {
        "id": "ZRNTszLW-SXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь проходим по предложениям из объекта `doc`, создавая отдельный список токенов для каждого предложения"
      ],
      "metadata": {
        "id": "TAJj4jdL-lYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'A severe storm hit the beach. It started to rain.')\n",
        "for sent in doc.sents:\n",
        "  print([sent[i] for i in range(len(sent))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lef28Fta95Fp",
        "outputId": "43cfe8bf-45e2-461d-f3ab-5d67431dc547"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A, severe, storm, hit, the, beach, .]\n",
            "[It, started, to, rain, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В то же время можно ссылаться на токены в состоящем из множества предложений тексте с помощью глобальных индексов уровня документа, как показано вот здесь:"
      ],
      "metadata": {
        "id": "KVTLtlUE-sDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYoDKSAh-dmp",
        "outputId": "2e509af8-97d0-4f0b-e277-739ebbd3f156"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возможность ссылаться на объекты `Token` в документе по их индексам уровня предложения удобна, когда нужно, например, проверить, является ли первое слово во втором предложении обрабатываемого текста местоимением"
      ],
      "metadata": {
        "id": "9D-qfAxv-0kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,sent in enumerate(doc.sents):\n",
        "  if i==1 and sent[0].pos_== 'PRON':\n",
        "    print('The second sentence begins with a pronoun.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQwSEzMI-vTo",
        "outputId": "eaff78f9-7201-45c4-ca20-ef8f5933e7ab"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second sentence begins with a pronoun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбор первого слова в предложении — элементарная задача, поскольку его индекс всегда равен 0. А как насчет последнего? Например, что делать, если необходимо определить, сколько предложений в тексте оканчивается глаголом (не считая точек и прочих знаков препинания)?"
      ],
      "metadata": {
        "id": "HCVlgl8m_CyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хотя длины предложений различны, их можно легко вычислить с помощью функции `len()`. Вычитаем 2 из значения `len(sent)` по следующим причинам: во-первых, индексы всегда начинаются с 0 и заканчиваются на *size-1*, во-вторых, последний токен в обоих предложениях нашего примера текста — точка, которую не нужно учитывать."
      ],
      "metadata": {
        "id": "2OB1O1DT_PxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for sent in doc.sents:\n",
        "  if sent[len(sent)-2].pos_ == 'VERB':\n",
        "    counter+=1\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQYcC31T-6g3",
        "outputId": "54b5971f-d30f-480d-9850-53d5a5c4f2e4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Контейнер `doc.noun_chunks`"
      ],
      "metadata": {
        "id": "U1VOz7Bx_cEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью свойства `doc.noun_chunks` объекта `Doc` можно пройти по именным фрагментам. Именной фрагмент (*noun chunk*) — это фраза, главным элементом которой является существительное."
      ],
      "metadata": {
        "id": "f2wacGlE_iKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjB3lsEg_KlZ",
        "outputId": "565093bc-4393-49a1-fa32-927372c7f702"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A noun chunk\n",
            "a phrase\n",
            "that\n",
            "a noun\n",
            "its head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Или же для извлечения именных фрагментов можно обойти в цикле существительные в предложении и найти синтаксические дочерние элементы каждого существительного, чтобы из них образовать именные фрагменты."
      ],
      "metadata": {
        "id": "moK7U90n_xYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  if token.pos_=='NOUN':\n",
        "    chunk = ''\n",
        "    for w in token.children:\n",
        "       if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
        "         chunk = chunk + w.text + ' '\n",
        "    chunk = chunk + token.text\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBkb6_X7_tED",
        "outputId": "333926e5-faba-4055-8d71-37f391321386"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noun\n",
            "A chunk\n",
            "a phrase\n",
            "a noun\n",
            "head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Объект Span"
      ],
      "metadata": {
        "id": "5NCKy-61PJHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объект *Span* (от англ. span — «интервал») представляет собой часть объекта `Doc`."
      ],
      "metadata": {
        "id": "B8COShR6PMG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp('I want a green apple.')\n",
        "doc[2:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhcbGUJs_6PO",
        "outputId": "7a687245-f218-4445-e0e4-015ece520827"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "a green apple"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объект Span включает несколько методов, самый интересный из которых — `span.merge()`. С его помощью интервал можно объединять в единый токен, производя повторную токенизацию документа."
      ],
      "metadata": {
        "id": "Omj1ZBejPfXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n",
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUExFTyvBDD1",
        "outputId": "4890fa5b-0e76-4a7f-d7ec-01b36a5c0674"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждому слову и знаку препинания соответствует отдельный токен.\n",
        "\n",
        "С помощью метода `span.merge()` можно изменить поведение по умолчанию:"
      ],
      "metadata": {
        "id": "rcuIzj-sP3pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Поправка *Spacy* отказался от этого `span.merge()`метода с тех пор, как был создан этот учебник. Сейчас это можно сделать с помощью `doc.retokenize()`: https://spacy.io/api/doc#retokenize . \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NjJtD0IFULWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with doc.retokenize() as retokenizer:\n",
        "  attrs = {\"LEMMA\": \"Golden Gate Bridge\"}\n",
        "  retokenizer.merge(doc[1:4], attrs=attrs)\n",
        "with doc.retokenize() as retokenizer:\n",
        "  attrs = {\"LEMMA\": \"San Francisco\"}\n",
        "  retokenizer.merge(doc[7:9], attrs=attrs)\n",
        "[doc[i] for i in range(len(doc))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO67pzfXP0j7",
        "outputId": "d1a4c7bd-1d7c-4e9d-d34b-a300dfde4cf8"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The, Golden Gate Bridge, is, an, iconic, landmark, in, San Francisco, .]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(\"{:<18}\\t{:<18}\\t{:<5}\\t{:<5}\".format(token.text, token.lemma_, token.pos_, token.dep_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm-o3UBgP9rv",
        "outputId": "bae7fd2f-56b9-49f3-d92d-da3e0b8df556"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The               \tthe               \tDET  \tdet  \n",
            "Golden Gate Bridge\tGolden Gate Bridge\tPROPN\tnsubj\n",
            "is                \tbe                \tAUX  \tROOT \n",
            "an                \tan                \tDET  \tdet  \n",
            "iconic            \ticonic            \tADJ  \tamod \n",
            "landmark          \tlandmark          \tNOUN \tattr \n",
            "in                \tin                \tADP  \tprep \n",
            "San Francisco     \tSan Francisco     \tPROPN\tpobj \n",
            ".                 \t.                 \tPUNCT\tpunct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Настройка конвейера обработки текста под свои нужды"
      ],
      "metadata": {
        "id": "1j-1OPzXYGwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотреть доступные для объекта `nlp` компоненты конвейера можно с помощью команды:"
      ],
      "metadata": {
        "id": "H_xoqdtdYT_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMZKk5oLRvfX",
        "outputId": "0fccc18e-af37-498e-d6ca-25b8f30dfe77"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Отключение компонентов конвейера"
      ],
      "metadata": {
        "id": "o5RH_y7TY9ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser'])"
      ],
      "metadata": {
        "id": "xRWAQ4UcYW2d"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном случае мы создадим конвейер обработки без утилиты разбора зависимостей. При вызове такого экземпляра `nlp` для конкретного текста токены в этом тексте не получат метки зависимостей."
      ],
      "metadata": {
        "id": "3o6KdJIvZX-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I want a green apple.')\n",
        "for token in doc:\n",
        "  print(\"{:<6}\\t{:<6}\\t{:<5}\".format(token.text, token.pos_, token.dep_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRPBzz23Zcb-",
        "outputId": "d902bed2-ad72-452f-acab-adaaaaa4df65"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I     \tPRON  \t     \n",
            "want  \tVERB  \t     \n",
            "a     \tDET   \t     \n",
            "green \tADJ   \t     \n",
            "apple \tNOUN  \t     \n",
            ".     \tPUNCT \t     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Пошаговая загрузка модели"
      ],
      "metadata": {
        "id": "dlYg7u1NZzC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "fvJ-IjK8Zd7U"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можете выяснить, где именно модель находится в вашей системе. Поможет вспомогательная функция `get_package_path`:"
      ],
      "metadata": {
        "id": "y5UjplhIaJcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import util\n",
        "util.get_package_path('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxVdPiUDZ4Mx",
        "outputId": "55185cb8-f814-4fab-ed6e-a887a07903ff"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/usr/local/lib/python3.7/dist-packages/en_core_web_sm')"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим модель и версию"
      ],
      "metadata": {
        "id": "EEStr2UVaans"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.meta['version'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxWwNuehaIRY",
        "outputId": "a8d7701b-648c-49c6-b823-a59d4bafd480"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_core_web_sm-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Иногда полезно взглянуть на список компонентов конвейера, используемых с моделью. Список можно получить из поля `pipeline` атрибута `nlp.meta`"
      ],
      "metadata": {
        "id": "fIuxO5xhai9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.meta['pipeline']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aPXn3rYaXUL",
        "outputId": "81764e7f-f7e2-4542-c9d6-6462f337ba6c"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*код не работает*"
      ],
      "metadata": {
        "id": "rQFWpwOnil6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang = 'en'\n",
        "pipeline = ['tagger', 'parser', 'ner']\n",
        "model_data_path = '/usr/local/lib/python3.7/dist-packages/en_core_web_sm-3.4.0'\n",
        "lang_cls = spacy.util.get_lang_class(lang)\n",
        "nlp = lang_cls()\n",
        "for name in pipeline:\n",
        "  # component = nlp.create_pipe(name)\n",
        "  nlp.add_pipe(name, name = name)\n",
        "nlp.from_disk(model_data_path, exclude=pipeline)"
      ],
      "metadata": {
        "id": "2WJs6PfPbfOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Настройка компонентов конвейера под свои нужды"
      ],
      "metadata": {
        "id": "KEHfNKdIdnKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70slBvjWcEVu",
        "outputId": "7d55ebe4-5aae-4b12-9c9b-2416acdf9abc"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy WORK_OF_ART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метка `WORK_OF_AR` обозначает произведение искусства. Но нам нужно, чтобы средство распознавания сущностей классифицировало его как сущность типа `DISTRICT`.\n",
        "\n",
        "Сначала добавим новую метку `DISTRICT` в список поддерживаемых типов сущностей."
      ],
      "metadata": {
        "id": "6X00AYtqeLni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL = 'DISTRICT'\n",
        "TRAIN_DATA = [\n",
        "    ('We need to deliver it to Festy.', {\n",
        "        'entities': [(25, 30, 'DISTRICT')]\n",
        "    }),\n",
        "    ('I like red oranges', {\n",
        "        'entities': []\n",
        "    })\n",
        "  ]"
      ],
      "metadata": {
        "id": "HDQYVsefduzY"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий этап — добавление новой метки сущности `DISTRICT` в компонент распознавания сущностей."
      ],
      "metadata": {
        "id": "zPDqhXFze5IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "id": "FkZsKz0re8BJ"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполнив этот шаг, в полученный объект `ner` можно добавить новую метку с помощью метода `ner.add_label()`:"
      ],
      "metadata": {
        "id": "EP0DgBpje_LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner.add_label(LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWx3Q8bte9Zh",
        "outputId": "5e8d111d-4571-4cb7-d72d-bbfd954935bd"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде чем приступить к обучению средства распознавания сущностей, необходимо отключить остальные конвейеры, чтобы во время обучения обновлялся только компонент распознавания сущностей:"
      ],
      "metadata": {
        "id": "7FEF5AFFfFWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.disable_pipes('tagger')\n",
        "nlp.disable_pipes('parser')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI6WKiPVfCv0",
        "outputId": "5a2d9438-e00f-4219-b2f3-4756bc953f3d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parser']"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь можно начинать обучение компонента распознаванию сущностей на примерах данных из списка `TRAIN_DATA`, который был создан ранее в этом разделе:"
      ],
      "metadata": {
        "id": "fm8DNBE9fKId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*код исправлен*"
      ],
      "metadata": {
        "id": "X3iQI4qSigbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.create_optimizer()\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "for i in range(25):\n",
        "  random.shuffle(TRAIN_DATA)\n",
        "  for text, annotations in TRAIN_DATA:\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    nlp.update([example], sgd=optimizer)"
      ],
      "metadata": {
        "id": "fvJrIHuFfIEw"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По завершении выполнения можно проверить, как обновленный оптимизатор распознает токен Festy:"
      ],
      "metadata": {
        "id": "7IZr4XDIi2zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*не вышло*"
      ],
      "metadata": {
        "id": "Y0lk1v03jE-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'I need a taxi to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw3T_85CfP6J",
        "outputId": "c4718ed5-a046-475a-d31a-81d9d279548f"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy WORK_OF_ART\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь при необходимости можно загрузить обновленный компонент в новом сеансе, используя метод from_disk(). Чтобы убедиться в этом, закройте текущий сеанс интерпретатора, откройте новый и выполните следующий код:\n",
        "\n",
        "\n",
        "*не работает*"
      ],
      "metadata": {
        "id": "PvD-TXIej6PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner.to_disk('/usr/to/ner')"
      ],
      "metadata": {
        "id": "6FxCY5egkan5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.pipeline import EntityRecognizer\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "ner = EntityRecognizer(nlp.vocab)\n",
        "ner.from_disk('/usr/to/ner')\n",
        "nlp.add_pipe(ner)"
      ],
      "metadata": {
        "id": "o0fB6xssjKyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'We need to deliver it to Festy.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSKvdxE4jWLs",
        "outputId": "3eeb5028-c17b-4fef-ea1b-873c0aa719e0"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Festy WORK_OF_ART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Использование структур данных уровня языка С библиотеки spaCy"
      ],
      "metadata": {
        "id": "cMtvYBA3ky3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Подготовка рабочей среды и получение текстовых файлов"
      ],
      "metadata": {
        "id": "_d5jSP8fk8BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-uT18rFksJp",
        "outputId": "020dcbcc-71fc-4334-c7b5-4c12af10c2e6"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Сценарий Cypthon"
      ],
      "metadata": {
        "id": "EvydKp3TlI_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем в одном из каталогов локальной файловой системы файл `spacytext.pyx` и вставляем в него следующий код:"
      ],
      "metadata": {
        "id": "ZgJL6jEAlPz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('spacytext.pyx', 'w')"
      ],
      "metadata": {
        "id": "S62lmv-PpGwm"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.write(\"\"\"from cymem.cymem cimport Pool\n",
        "from spacy.tokens.doc cimport Doc\n",
        "from spacy.structs cimport TokenC\n",
        "from spacy.typedefs cimport hash_t\n",
        "\n",
        "cdef struct DocStruct:\n",
        "  TokenC* c\n",
        "  int length\n",
        "\n",
        "cdef int counter(DocStruct* doc, hash_t tag):\n",
        "  cdef int cnt = 0\n",
        "  for c in doc.c[:doc.length]:\n",
        "    if c.tag == tag:\n",
        "      cnt += 1\n",
        "  return cnt\n",
        "\n",
        "cpdef main(Doc mydoc):\n",
        "  cdef int cnt\n",
        "  cdef Pool mem = Pool()\n",
        "  cdef DocStruct* doc_ptr = <DocStruct*>mem.alloc(1, sizeof(DocStruct))\n",
        "  doc_ptr.c = mydoc.c\n",
        "  doc_ptr.length = mydoc.length\n",
        "  tag = mydoc.vocab.strings.add('PRP')\n",
        "  cnt = counter(doc_ptr, tag)\n",
        "  print(doc_ptr.length)\n",
        "  print(cnt)\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMeh10gklC10",
        "outputId": "99897862-6e15-4e03-f460-b943adde46db"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "623"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.close()"
      ],
      "metadata": {
        "id": "17QqlKdwpZoe"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Сборка модуля Cython"
      ],
      "metadata": {
        "id": "NDZwHdo6oiLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем файл `setup.py` в каталоге, где располагается наш сценарий Cython. Файл должен содержать следующий код:"
      ],
      "metadata": {
        "id": "JzQ0hK8-qOe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('setup.py', 'w')"
      ],
      "metadata": {
        "id": "Iy90a6fyqSHd"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.write('''from distutils.core import setup\n",
        "from Cython.Build import cythonize\n",
        "\n",
        "import numpy\n",
        "setup(name='spacy text app',\n",
        "      ext_modules=cythonize(\"spacytext.pyx\", language=\"c++\"),\n",
        "      include_dirs=[numpy.get_include()]\n",
        "      )''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2s0_gJ1oflf",
        "outputId": "14249f9e-698a-4540-921c-4642b57805ad"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.close()"
      ],
      "metadata": {
        "id": "R_Rm6qDlofiY"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После подготовки установочного сценария компилируем код Cython.\n",
        "Сделать это можно из системного терминала:"
      ],
      "metadata": {
        "id": "V2_cxToSqkoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOpympwSqiv6",
        "outputId": "30d04c98-c4dc-438f-a26d-877724300b61"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: passing language='c++' to cythonize() is deprecated. Instead, put \"# distutils: language=c++\" in your .pyx or .pxd file(s)\n",
            "running build_ext\n",
            "building 'spacytext' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-cpython-37\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c spacytext.cpp -o build/temp.linux-x86_64-cpython-37/spacytext.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kspacytext.cpp:774\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-37\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-37/spacytext.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-37/spacytext.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-cpython-37/spacytext.cpython-37m-x86_64-linux-gnu.so -> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Тестирование модуля"
      ],
      "metadata": {
        "id": "ZjXSp8tUqnuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После успешного завершения процесса компиляции модуль `spacytext` будет добавлен в среду Python. Для его тестирования откройте сеанс Python и выполните команду:"
      ],
      "metadata": {
        "id": "0HCePGOQqy8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacytext import main"
      ],
      "metadata": {
        "id": "kcbPebJoqrPs"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "f= open(\"test.txt\",\"rb\")\n",
        "contents =f.read()\n",
        "doc = nlp(contents[:100000].decode('utf8'))\n",
        "main(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YjtLGDAq5_d",
        "outputId": "5845ed3d-1f02-4ac9-da3a-3d1fe1ab7bac"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Выделение и использование лингвистических признаков"
      ],
      "metadata": {
        "id": "UdaVFcADsVF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Теги для чисел, символов и знаков препинания"
      ],
      "metadata": {
        "id": "ZnCejsf7soFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала выделим из токенов признаки общих частей речи и увидим, как spaCy распознает различные части речи:"
      ],
      "metadata": {
        "id": "zghZyvFptRMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы создали для входного предложения объект Doc и вывели теги общих частей речи, а также воспользовались функцией `spacy.explain()`, которая возвращает описание для заданного лингвистического признака."
      ],
      "metadata": {
        "id": "2-pVLUZWtYkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
        "for token in doc:\n",
        "  print(\"{:10}\\t{:10}\\t{}\".format(token.text, token.pos_, spacy.explain(token.pos_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K1G4xKFq9Z2",
        "outputId": "dde30481-1245-4c38-aa52-2882c60de001"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The       \tDET       \tdeterminer\n",
            "firm      \tNOUN      \tnoun\n",
            "earned    \tVERB      \tverb\n",
            "$         \tSYM       \tsymbol\n",
            "1.5       \tNUM       \tnumeral\n",
            "million   \tNUM       \tnumeral\n",
            "in        \tADP       \tadposition\n",
            "2017      \tNUM       \tnumeral\n",
            ".         \tPUNCT     \tpunctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь сравним теги общих и уточненных частей речи для того же предложения, выведя в отдельном столбце описание для тегов уточненных частей речи:"
      ],
      "metadata": {
        "id": "iKhM0qwVteu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Второй и третий столбцы содержат теги общих и уточненных частей речи соответственно. В четвертом столбце приведено описание тегов уточненных частей речи из третьего столбца."
      ],
      "metadata": {
        "id": "J2LghUobtkGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(\"{:10}\\t{:10}\\t{:3}\\t{}\".format(token.text, token.pos_, token.tag_, spacy.explain(token.tag_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRfHrEg8szEk",
        "outputId": "1ebdd6db-f1b8-40df-ee21-d5ac004cf6db"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The       \tDET       \tDT \tdeterminer\n",
            "firm      \tNOUN      \tNN \tnoun, singular or mass\n",
            "earned    \tVERB      \tVBD\tverb, past tense\n",
            "$         \tSYM       \t$  \tsymbol, currency\n",
            "1.5       \tNUM       \tCD \tcardinal number\n",
            "million   \tNUM       \tCD \tcardinal number\n",
            "in        \tADP       \tIN \tconjunction, subordinating or preposition\n",
            "2017      \tNUM       \tCD \tcardinal number\n",
            ".         \tPUNCT     \t.  \tpunctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Выделение описаний денежных сумм"
      ],
      "metadata": {
        "id": "XuUID8k3tmNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий сценарий иллюстрирует, как фразу \"$1.5 million\" можно выделить из предложения на основе одних лишь тегов частей речи токенов. Можете сохранить этот сценарий в файле и затем выполнить код из сеанса Python:"
      ],
      "metadata": {
        "id": "ZvRvDnl3tr4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом коде проходим по токенам предложения в цикле в поисках токена с тегом $ уточненной части речи. Данный тег обозначает символ валюты, с которого обычно начинается фраза, описывающая некоторую сумму денег."
      ],
      "metadata": {
        "id": "g6bhzyfwuCAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"The firm earned $1.5 million in 2017.\")\n",
        "phrase = ''\n",
        "for token in doc:\n",
        "  if token.tag_ == '$':\n",
        "    phrase = token.text\n",
        "    i = token.i+1\n",
        "    while doc[i].tag_ == 'CD':\n",
        "      phrase += doc[i].text + ' '\n",
        "      i += 1\n",
        "    break\n",
        "phrase = phrase[:-1]\n",
        "print(phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlhEYpMgtJv1",
        "outputId": "d12cf193-bad1-49e0-c02a-6cbef2e6f8af"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$1.5 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Преобразование утвердительных высказываний в вопросительные"
      ],
      "metadata": {
        "id": "rZMlPWqruKJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предложение содержит несколько глаголов и местоимений, причем с различной морфологией. Чтобы в этом убедиться, взглянем на теги частей речи, которые spaCy присваивает токенам предложения:"
      ],
      "metadata": {
        "id": "xpdQK5uYuSbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  print(\"{:7}\\t{:5}\\t{}\".format(token.text, token.pos_, token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7lfnqwQt3_K",
        "outputId": "236a95d9-66fe-425a-8e4e-a149551e1647"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I      \tPRON \tPRP\n",
            "can    \tAUX  \tMD\n",
            "promise\tVERB \tVB\n",
            "it     \tPRON \tPRP\n",
            "is     \tAUX  \tVBZ\n",
            "worth  \tADJ  \tJJ\n",
            "your   \tPRON \tPRP$\n",
            "time   \tNOUN \tNN\n",
            ".      \tPUNCT\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные шаги генерации вопроса из исходного утверждения."
      ],
      "metadata": {
        "id": "xIiCzRNwuxIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Поменять порядок слов в исходном предложении с «подлежащее + вспомогательный модальный глагол + глагол в неопределенной форме» на «модальный вспомогательный глагол + глагол в неопределенной форме + подлежащее».\n",
        "2. Заменить личное местоимение I (подлежащее в предложении) на you.\n",
        "3. Заменить притяжательное местоимение your на my.\n",
        "4. Вставить наречие-модификатор really перед словом promise для усиления последнего.\n",
        "5. Заменить знак препинания . на ? в конце предложения."
      ],
      "metadata": {
        "id": "fak1QlAZu0cN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эти шаги реализованы в следующем сценарии:"
      ],
      "metadata": {
        "id": "IpSupwsJvAvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGvmyORWv8t-",
        "outputId": "ae6708dd-6386-4cfd-aad2-7e07ee0bf748"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I can promise it is worth your time."
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде всего проходим в цикле по токенам предложения и меняем местами \n",
        "существительное и глагол, чтобы предложение стало вопросительным."
      ],
      "metadata": {
        "id": "t4K0WY5jv_An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "sent = ''\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':\n",
        "    sent = doc[i+1].text.capitalize() + ' ' + doc[i].text\n",
        "    sent = sent + ' ' + doc[i+2:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Fqwuz7CJuWAG",
        "outputId": "c2f44003-e660-4d7b-83ae-0f96d42be644"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can I promise it is worth your time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее создаем новый цикл `for`, который заменит личное местоимение `I` личным местоимением you. Для этого ищем личные местоимения (помеченные тегами `PRP`). Если личное местоимение — `I`, меняем его на `you` и выходим из цикла `for`."
      ],
      "metadata": {
        "id": "SgqV6lqHwIy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP' and token.text == 'I':\n",
        "    print(doc[:i].text)\n",
        "    sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6mmbX_0yvQRC",
        "outputId": "48815dd2-acb3-4c85-a557-86e20c7e87a2"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you promise it is worth your time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Повторяем этот процесс. Ищем тег `PRP$` и меняем притяжательное местоимение `your` на `my`."
      ],
      "metadata": {
        "id": "Q0BF892XxoXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'PRP$' and token.text == 'your':\n",
        "    sent = doc[:i].text + ' my ' + doc[i+1:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2ilPNX2xvUxc",
        "outputId": "5077c859-a628-44d2-e8a8-d70b597e3d45"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you promise it is worth my time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В новом цикле `for` находим глагол в неопределенной форме и вставляем перед ним наречие-модификатор `really`"
      ],
      "metadata": {
        "id": "I-m9k2cVxtga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "for i,token in enumerate(doc):\n",
        "  if token.tag_ == 'VB':\n",
        "    sent = doc[:i].text + ' really ' + doc[i:].text\n",
        "    break\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G6AT9-ovvbLL",
        "outputId": "bd89ba41-357c-4168-c03b-ae50dee71de4"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you really promise it is worth my time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наконец, заменяем точку в конце предложения вопросительным знаком: это единственный шаг, для которого не нужен цикл."
      ],
      "metadata": {
        "id": "4IP2wmupxzdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(sent)\n",
        "sent = doc[:len(doc)-1].text + '?'\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "30Nvs_ntvhOR",
        "outputId": "1487a077-7487-4ae9-da82-9a99de37c5ba"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can I promise it is worth your time?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Использование меток синтаксических зависимостей при обработке языка"
      ],
      "metadata": {
        "id": "R2uoE78YyCPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Различаем подлежащие и дополнения"
      ],
      "metadata": {
        "id": "L_A8OdAVyKVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы определить программным образом, чем в заданном предложении являются такие местоимения, как you или it, необходимо посмотреть на присвоенную им метку зависимости. Теги частей речи в сочетании с метками зависимостей позволяют получить гораздо больше информации о роли токена в предложении."
      ],
      "metadata": {
        "id": "H94IBOudyOLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Второй и третий столбцы содержат теги общих и уточненных частей речи соответственно. Четвертый столбец содержит метки зависимостей, а пятый — описания этих меток."
      ],
      "metadata": {
        "id": "kIx_sjdLys84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I can promise it is worth your time.\")\n",
        "for token in doc:\n",
        "  print(\"{:5}\\t{:5}\\t{:5}\\t{:10}\\t{}\".format(token.text, token.pos_, token.tag_, token.dep_, spacy.explain(token.dep_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPJ-SeVcvk3O",
        "outputId": "cf8acbee-5e46-4bdc-b93c-a462bdd7ebc0"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I    \tPRON \tPRP  \tnsubj     \tnominal subject\n",
            "can  \tAUX  \tMD   \taux       \tauxiliary\n",
            "promise\tVERB \tVB   \tROOT      \troot\n",
            "it   \tPRON \tPRP  \tnsubj     \tnominal subject\n",
            "is   \tAUX  \tVBZ  \tccomp     \tclausal complement\n",
            "worth\tADJ  \tJJ   \tacomp     \tadjectival complement\n",
            "your \tPRON \tPRP$ \tposs      \tpossession modifier\n",
            "time \tNOUN \tNN   \tnpadvmod  \tnoun phrase as adverbial modifier\n",
            ".    \tPUNCT\t.    \tpunct     \tpunctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Выясняем, какой вопрос далжен задать чат-бот"
      ],
      "metadata": {
        "id": "dz6K_NXPyx7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнем с импорта модуля sys, который позволяет получить предложение в виде аргумента для дальнейшей обработки:"
      ],
      "metadata": {
        "id": "Jabw0bF3zI1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import sys\n",
        "from spacy.tokens.doc import Doc\n",
        "from spacy.vocab import Vocab"
      ],
      "metadata": {
        "id": "E6xOL-t-yYVK"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее опишем функцию для распознавания и извлечения произвольного именного фрагмента — прямого дополнения из входного документа. Например, если вы ввели документ, содержащий предложение *I want a green apple.*, то будет возвращен фрагмент *a green apple*:"
      ],
      "metadata": {
        "id": "La-5tTPYzM7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chunk(doc):\n",
        "  chunk = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj':\n",
        "      shift = len([w for w in token.children])\n",
        "      #print([w for w in token.children])\n",
        "      сhunk = doc[i-shift:i+1]\n",
        "      break\n",
        "  return chunk"
      ],
      "metadata": {
        "id": "iQx7EhvDy9sm"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция просматривает фрагмент и определяет, какой тип вопроса должен задать чат-бот:"
      ],
      "metadata": {
        "id": "kAP1Qx3jzTWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала задаем начальное значение переменной `question_type` равным 'yesno', что соответствует вопросу типа «да/нет». Далее в переданном в функцию `chunk` ищем токен с тегом 'amod' который означает прилагательное-модификатор. Если таковое находится, меняем значение переменной `question_type` на 'info', соответствующее информационному типу вопроса."
      ],
      "metadata": {
        "id": "ZbyDoHvrzknH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_question_type(chunk):\n",
        "  question_type = 'yesno'\n",
        "  for token in chunk:\n",
        "    if token.dep_ == 'amod':\n",
        "      question_type = 'info'\n",
        "  return question_type"
      ],
      "metadata": {
        "id": "UhR8E8vgzU6M"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определив, какой тип вопроса нам нужен, генерируем в следующей функции вопрос на основе входного предложения:"
      ],
      "metadata": {
        "id": "Ka6MQA3ezfcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(doc, question_type):\n",
        "  sent = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
        "      sent = 'do ' + doc[i].text\n",
        "      sent = sent + ' ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and token.text == 'I':\n",
        "      sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  if question_type == 'info':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
        "        break\n",
        "  if question_type == 'yesno':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
        "        break\n",
        "  doc=nlp(sent)\n",
        "  sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'\n",
        "  return sent"
      ],
      "metadata": {
        "id": "r-qcPiGF0BiE"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратите внимание: используемый алгоритм предполагает, что входное предложение оканчивается знаком препинания, например . или !.\n",
        "\n",
        "После описания всех функций посмотрим на основной блок сценария:"
      ],
      "metadata": {
        "id": "tWAP4sWZ0V6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chunk(doc):\n",
        "  chunk = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj':\n",
        "      shift = len([w for w in token.children])\n",
        "      chunk = doc[i-shift:i+1]\n",
        "      break\n",
        "  # print(chunk)\n",
        "  return chunk\n",
        "\n",
        "def determine_question_type(chunk):\n",
        "  question_type = 'yesno'\n",
        "  for token in chunk:\n",
        "    if token.dep_ == 'amod':\n",
        "      question_type = 'info'\n",
        "  return question_type\n",
        "\n",
        "def generate_question(doc, question_type):\n",
        "  sent = ''\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and doc[i+1].tag_ == 'VBP':\n",
        "      sent = 'do ' + doc[i].text\n",
        "      sent = sent + ' ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  for i,token in enumerate(doc):\n",
        "    if token.tag_ == 'PRP' and token.text == 'I':\n",
        "      sent = doc[:i].text + ' you ' + doc[i+1:].text\n",
        "      break\n",
        "  doc=nlp(sent)\n",
        "  if question_type == 'info':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text\n",
        "        break\n",
        "  if question_type == 'yesno':\n",
        "    for i,token in enumerate(doc):\n",
        "      if token.dep_ == 'dobj':\n",
        "        sent = doc[:i-1].text + ' a red ' + doc[i:].text\n",
        "        break\n",
        "  doc=nlp(sent)\n",
        "  sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'\n",
        "  return sent\n",
        "\n",
        "def chat_bot(new_str):\n",
        "  if len(new_str) > 1:\n",
        "    sent = new_str\n",
        "    # print(\"Sent: \" + sent)\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(sent)\n",
        "    # print(f\"Doc: {doc}\")\n",
        "    chunk = find_chunk(doc)\n",
        "    # print(f\"Chunk: {chunk}\")\n",
        "    if chunk == None:\n",
        "      print('The sentence does not contain a direct object.')\n",
        "      sys.exit()\n",
        "    question_type = determine_question_type(chunk)\n",
        "    question = generate_question(doc, question_type)\n",
        "    print(question)\n",
        "  else:\n",
        "    print('You did not submit a sentence!')\n",
        "\n",
        "\n",
        "s1 = 'I want a green apple.'\n",
        "print(s1)\n",
        "chat_bot(s1)\n",
        "s2 = 'I want an apple.'\n",
        "print(s2)\n",
        "chat_bot(s2)\n",
        "s3 = 'I want...'\n",
        "print(s3)\n",
        "chat_bot(s3)\n",
        "s4 = \"\"\n",
        "print(s4)\n",
        "chat_bot(s4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC3yXNa-0Y9C",
        "outputId": "8d292958-e3e2-46e1-ac21-495544ea13f2"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want a green apple.\n",
            "Why do you want a green one?\n",
            "I want an apple.\n",
            "Do you want a red apple?\n",
            "I want...\n",
            "Do you want?\n",
            "\n",
            "You did not submit a sentence!\n"
          ]
        }
      ]
    }
  ]
}